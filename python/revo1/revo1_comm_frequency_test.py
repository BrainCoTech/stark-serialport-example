"""
Revo1çµå·§æ‰‹é€šè®¯é¢‘ç‡æµ‹è¯•

æœ¬æµ‹è¯•ä¸“é—¨ç”¨äºè¯„ä¼°Revo1çµå·§æ‰‹çš„å®é™…ä½¿ç”¨åœºæ™¯ä¸‹çš„é€šè®¯é¢‘ç‡å’Œæ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- get_motor_status è¯»å–é¢‘ç‡æµ‹è¯•ï¼ˆçŠ¶æ€ç›‘æ§ï¼‰
- set_finger_positions è®¾ç½®é¢‘ç‡æµ‹è¯•ï¼ˆä½ç½®æ§åˆ¶ï¼‰
- ä¸åŒé€šè®¯åŠŸèƒ½çš„å»¶è¿Ÿåˆ†æ
- é«˜é¢‘è¿ç»­é€šè®¯çš„ç¨³å®šæ€§æµ‹è¯•
- é€šè®¯é”™è¯¯ç‡ç»Ÿè®¡

æµ‹è¯•åŠŸèƒ½ï¼š
1. å•ä¸€åŠŸèƒ½é¢‘ç‡æµ‹è¯•ï¼šè¿ç»­è°ƒç”¨åŒä¸€APIæµ‹é‡æœ€å¤§é¢‘ç‡
2. æ··åˆåŠŸèƒ½æµ‹è¯•ï¼šæ¨¡æ‹Ÿå®é™…ä½¿ç”¨åœºæ™¯çš„è¯»å–+æ§åˆ¶è°ƒç”¨
3. é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•ï¼šè¯„ä¼°é•¿æœŸè¿è¡Œçš„ç¨³å®šæ€§
4. æ•°æ®å®Œæ•´æ€§éªŒè¯ï¼šç¡®ä¿é«˜é¢‘é€šè®¯ä¸‹æ•°æ®çš„å‡†ç¡®æ€§

åº”ç”¨åœºæ™¯ï¼š
- å®æ—¶æ§åˆ¶ç³»ç»Ÿæ€§èƒ½è¯„ä¼°
- æ‰‹æŒ‡ä½ç½®æ§åˆ¶é¢‘ç‡æµ‹è¯•
- çŠ¶æ€ç›‘æ§ä¸æ§åˆ¶çš„åè°ƒæ€§æµ‹è¯•
- ç³»ç»Ÿé›†æˆæµ‹è¯•
"""

import asyncio
import time
import statistics
import sys
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from revo1_utils import *
import matplotlib.pyplot as plt
import json

# æµ‹è¯•é…ç½®å‚æ•°
@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®ç±»"""
    # åŸºç¡€æµ‹è¯•å‚æ•°
    test_duration: float = 10.0      # å•ä¸ªæµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    max_test_count: int = 1000       # æœ€å¤§æµ‹è¯•æ¬¡æ•°
    timeout_ms: float = 100.0        # è¶…æ—¶é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰

    # é¢‘ç‡æµ‹è¯•å‚æ•°
    target_frequency: int = 100      # ç›®æ ‡é¢‘ç‡ï¼ˆHzï¼‰
    min_interval_ms: float = 1.0     # æœ€å°é—´éš”ï¼ˆæ¯«ç§’ï¼‰

    # ç¨³å®šæ€§æµ‹è¯•å‚æ•°
    stability_duration: float = 60.0 # ç¨³å®šæ€§æµ‹è¯•æ—¶é•¿ï¼ˆç§’ï¼‰
    sample_interval: float = 1.0     # é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰

@dataclass
class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    success: bool
    elapsed_ms: float
    timestamp: float
    data_size: int = 0
    error_msg: str = ""

@dataclass
class FrequencyTestReport:
    """é¢‘ç‡æµ‹è¯•æŠ¥å‘Š"""
    function_name: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    success_rate: float
    avg_latency_ms: float
    max_latency_ms: float
    min_latency_ms: float
    std_dev_ms: float
    actual_frequency_hz: float
    target_frequency_hz: float
    total_duration_s: float

class Revo1CommTester:
    """Revo1é€šè®¯é¢‘ç‡æµ‹è¯•å™¨"""

    def __init__(self, config: TestConfig):
        self.config = config
        self.client: Optional[libstark.PyDeviceContext] = None
        self.slave_id: Optional[int] = None
        self.test_results: Dict[str, List[TestResult]] = {}

    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥"""
        try:
            logger.info("ğŸ”Œ æ­£åœ¨è¿æ¥Revo1è®¾å¤‡...")
            self.client, self.slave_id = await open_modbus_revo1()
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°è®¾å¤‡ï¼Œä»ç«™ID: {self.slave_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.client:
            libstark.modbus_close(self.client)
            logger.info("ğŸ”Œ è¿æ¥å·²å…³é—­")

    async def test_get_motor_status_frequency(self) -> FrequencyTestReport:
        """æµ‹è¯• get_motor_status çš„è¯»å–é¢‘ç‡"""
        logger.info("ğŸ“Š å¼€å§‹æµ‹è¯• get_motor_status è¯»å–é¢‘ç‡...")

        if not self.client or self.slave_id is None:
            raise RuntimeError("è®¾å¤‡æœªåˆå§‹åŒ–")

        function_name = "get_motor_status"
        results = []
        start_time = time.perf_counter()
        test_count = 0

        while (time.perf_counter() - start_time) < self.config.test_duration and test_count < self.config.max_test_count:
            test_start = time.perf_counter()

            try:
                # è°ƒç”¨ get_motor_status
                status: libstark.MotorStatusData = await self.client.get_motor_status(self.slave_id)

                elapsed_ms = (time.perf_counter() - test_start) * 1000

                # è®°å½•æˆåŠŸç»“æœ
                result = TestResult(
                    success=True,
                    elapsed_ms=elapsed_ms,
                    timestamp=time.perf_counter() - start_time,
                    data_size=len(status.positions) if hasattr(status, 'positions') else 0
                )
                results.append(result)

                if test_count % 100 == 0:
                    logger.info(f"  ç¬¬ {test_count} æ¬¡æµ‹è¯•ï¼Œå»¶è¿Ÿ: {elapsed_ms:.2f}ms")

            except Exception as e:
                elapsed_ms = (time.perf_counter() - test_start) * 1000
                result = TestResult(
                    success=False,
                    elapsed_ms=elapsed_ms,
                    timestamp=time.perf_counter() - start_time,
                    error_msg=str(e)
                )
                results.append(result)
                logger.warning(f"  æµ‹è¯•å¤±è´¥: {e}")

            test_count += 1

            # æ§åˆ¶æœ€å°é—´éš”
            if self.config.min_interval_ms > 0:
                await asyncio.sleep(self.config.min_interval_ms / 1000)

        total_duration = time.perf_counter() - start_time

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(function_name, results, total_duration, self.config.target_frequency)
        self.test_results[function_name] = results

        return report

    async def test_set_finger_positions_frequency(self) -> FrequencyTestReport:
        """æµ‹è¯• set_finger_positions çš„è®¾ç½®é¢‘ç‡"""
        logger.info("ğŸ“Š å¼€å§‹æµ‹è¯• set_finger_positions è®¾ç½®é¢‘ç‡...")

        if not self.client or self.slave_id is None:
            raise RuntimeError("è®¾å¤‡æœªåˆå§‹åŒ–")

        function_name = "set_finger_positions"
        results = []
        start_time = time.perf_counter()
        test_count = 0

        # å®šä¹‰æµ‹è¯•ä½ç½®åºåˆ—ï¼šæ¡æ‰‹ -> å¼ å¼€ -> åŠæ¡ -> å¼ å¼€
        position_sequences = [
            [60, 60, 100, 100, 100, 100],  # æ¡æ‰‹
            [0, 0, 0, 0, 0, 0],            # å¼ å¼€
            [30, 30, 50, 50, 50, 50],      # åŠæ¡
            [0, 0, 0, 0, 0, 0],            # å¼ å¼€
        ]

        while (time.perf_counter() - start_time) < self.config.test_duration and test_count < self.config.max_test_count:
            test_start = time.perf_counter()

            try:
                # å¾ªç¯ä½¿ç”¨ä¸åŒçš„ä½ç½®è®¾ç½®
                positions = position_sequences[test_count % len(position_sequences)]

                # è°ƒç”¨ set_finger_positions
                await self.client.set_finger_positions(self.slave_id, positions)

                elapsed_ms = (time.perf_counter() - test_start) * 1000

                # è®°å½•æˆåŠŸç»“æœ
                result = TestResult(
                    success=True,
                    elapsed_ms=elapsed_ms,
                    timestamp=time.perf_counter() - start_time,
                    data_size=len(positions)
                )
                results.append(result)

                if test_count % 50 == 0:
                    logger.info(f"  ç¬¬ {test_count} æ¬¡æµ‹è¯•ï¼Œå»¶è¿Ÿ: {elapsed_ms:.2f}msï¼Œä½ç½®: {positions}")

            except Exception as e:
                elapsed_ms = (time.perf_counter() - test_start) * 1000
                result = TestResult(
                    success=False,
                    elapsed_ms=elapsed_ms,
                    timestamp=time.perf_counter() - start_time,
                    error_msg=str(e)
                )
                results.append(result)
                logger.warning(f"  æµ‹è¯•å¤±è´¥: {e}")

            test_count += 1

            # æ§åˆ¶é—´éš”ï¼Œä½ç½®è®¾ç½®ä¸éœ€è¦å¤ªé¢‘ç¹
            await asyncio.sleep(max(self.config.min_interval_ms / 1000, 0.005))  # æœ€å°‘5msé—´éš”

        total_duration = time.perf_counter() - start_time

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(function_name, results, total_duration, 50)  # ä½ç½®è®¾ç½®ç›®æ ‡é¢‘ç‡è¾ƒä½
        self.test_results[function_name] = results

        return report

    async def test_mixed_frequency(self) -> FrequencyTestReport:
        """æµ‹è¯•æ··åˆåŠŸèƒ½è°ƒç”¨é¢‘ç‡ï¼ˆæ¨¡æ‹Ÿå®é™…ä½¿ç”¨åœºæ™¯ï¼šè¯»å–çŠ¶æ€+æ§åˆ¶ä½ç½®ï¼‰"""
        logger.info("ğŸ“Š å¼€å§‹æµ‹è¯•æ··åˆåŠŸèƒ½è°ƒç”¨é¢‘ç‡...")

        if not self.client or self.slave_id is None:
            raise RuntimeError("è®¾å¤‡æœªåˆå§‹åŒ–")

        function_name = "mixed_functions"
        results = []
        start_time = time.perf_counter()
        test_count = 0

        # ä½ç½®åºåˆ—ï¼šæ¨¡æ‹Ÿå®é™…æŠ“å–åŠ¨ä½œ
        position_sequences = [
            [0, 0, 0, 0, 0, 0],            # å¼ å¼€
            [20, 20, 30, 30, 30, 30],      # é¢„å¤‡ä½ç½®
            [40, 40, 60, 60, 60, 60],      # æ¥è§¦
            [60, 60, 100, 100, 100, 100],  # æ¡ç´§
        ]

        while (time.perf_counter() - start_time) < self.config.test_duration and test_count < self.config.max_test_count:
            test_start = time.perf_counter()

            try:
                # æ¨¡æ‹Ÿå®é™…ä½¿ç”¨ï¼šä¸»è¦æ˜¯çŠ¶æ€è¯»å–ï¼Œé…åˆä½ç½®æ§åˆ¶
                if test_count % 5 == 0:
                    # æ¯5æ¬¡è°ƒç”¨ä¸€æ¬¡ä½ç½®è®¾ç½®
                    positions = position_sequences[test_count // 5 % len(position_sequences)]
                    await self.client.set_finger_positions(self.slave_id, positions)
                else:
                    # å¤§éƒ¨åˆ†æ—¶é—´è¯»å–çŠ¶æ€
                    await self.client.get_motor_status(self.slave_id)

                elapsed_ms = (time.perf_counter() - test_start) * 1000

                result = TestResult(
                    success=True,
                    elapsed_ms=elapsed_ms,
                    timestamp=time.perf_counter() - start_time
                )
                results.append(result)

                if test_count % 100 == 0:
                    action_type = "ä½ç½®è®¾ç½®" if test_count % 5 == 0 else "çŠ¶æ€è¯»å–"
                    logger.info(f"  ç¬¬ {test_count} æ¬¡æ··åˆæµ‹è¯•ï¼Œå»¶è¿Ÿ: {elapsed_ms:.2f}msï¼ŒåŠ¨ä½œ: {action_type}")

            except Exception as e:
                elapsed_ms = (time.perf_counter() - test_start) * 1000
                result = TestResult(
                    success=False,
                    elapsed_ms=elapsed_ms,
                    timestamp=time.perf_counter() - start_time,
                    error_msg=str(e)
                )
                results.append(result)
                logger.warning(f"  æ··åˆæµ‹è¯•å¤±è´¥: {e}")

            test_count += 1
            await asyncio.sleep(self.config.min_interval_ms / 1000)

        total_duration = time.perf_counter() - start_time

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(function_name, results, total_duration, self.config.target_frequency)
        self.test_results[function_name] = results

        return report

    async def test_stability(self) -> Dict[str, Any]:
        """é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•"""
        logger.info(f"ğŸ“Š å¼€å§‹é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯• ({self.config.stability_duration}ç§’)...")

        if not self.client or self.slave_id is None:
            raise RuntimeError("è®¾å¤‡æœªåˆå§‹åŒ–")

        stability_results = {
            'samples': [],
            'avg_latencies': [],
            'error_rates': [],
            'timestamps': []
        }

        start_time = time.perf_counter()
        sample_count = 0

        while (time.perf_counter() - start_time) < self.config.stability_duration:
            sample_start = time.perf_counter()
            sample_results = []

            # åœ¨é‡‡æ ·é—´éš”å†…è¿›è¡Œå¤šæ¬¡æµ‹è¯•
            while (time.perf_counter() - sample_start) < self.config.sample_interval:
                test_start = time.perf_counter()

                try:
                    await self.client.get_motor_status(self.slave_id)
                    elapsed_ms = (time.perf_counter() - test_start) * 1000
                    sample_results.append(TestResult(True, elapsed_ms, time.perf_counter() - start_time))
                except Exception as e:
                    elapsed_ms = (time.perf_counter() - test_start) * 1000
                    sample_results.append(TestResult(False, elapsed_ms, time.perf_counter() - start_time, error_msg=str(e)))

                await asyncio.sleep(0.01)  # 10msé—´éš”

            # è®¡ç®—æœ¬æ¬¡é‡‡æ ·çš„ç»Ÿè®¡æ•°æ®
            successful = [r for r in sample_results if r.success]
            total_count = len(sample_results)
            success_count = len(successful)

            avg_latency = statistics.mean([r.elapsed_ms for r in successful]) if successful else 0
            error_rate = (total_count - success_count) / total_count * 100 if total_count > 0 else 0

            stability_results['samples'].append(total_count)
            stability_results['avg_latencies'].append(avg_latency)
            stability_results['error_rates'].append(error_rate)
            stability_results['timestamps'].append(time.perf_counter() - start_time)

            sample_count += 1
            if sample_count % 10 == 0:
                logger.info(f"  ç¨³å®šæ€§æµ‹è¯•è¿›åº¦: {sample_count} ä¸ªé‡‡æ ·ç‚¹ï¼Œå½“å‰å»¶è¿Ÿ: {avg_latency:.2f}msï¼Œé”™è¯¯ç‡: {error_rate:.1f}%")

        return stability_results

    def _generate_report(self, function_name: str, results: List[TestResult],
                        total_duration: float, target_frequency: float) -> FrequencyTestReport:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        successful_results = [r for r in results if r.success]
        failed_results = [r for r in results if not r.success]

        total_tests = len(results)
        successful_tests = len(successful_results)
        failed_tests = len(failed_results)
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

        if successful_results:
            latencies = [r.elapsed_ms for r in successful_results]
            avg_latency = statistics.mean(latencies)
            max_latency = max(latencies)
            min_latency = min(latencies)
            std_dev = statistics.stdev(latencies) if len(latencies) > 1 else 0
        else:
            avg_latency = max_latency = min_latency = std_dev = 0

        actual_frequency = total_tests / total_duration if total_duration > 0 else 0

        return FrequencyTestReport(
            function_name=function_name,
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            avg_latency_ms=avg_latency,
            max_latency_ms=max_latency,
            min_latency_ms=min_latency,
            std_dev_ms=std_dev,
            actual_frequency_hz=actual_frequency,
            target_frequency_hz=target_frequency,
            total_duration_s=total_duration
        )

    def print_report(self, report: FrequencyTestReport):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {report.function_name} é¢‘ç‡æµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"æ€»æµ‹è¯•æ¬¡æ•°:     {report.total_tests}")
        print(f"æˆåŠŸæ¬¡æ•°:       {report.successful_tests}")
        print(f"å¤±è´¥æ¬¡æ•°:       {report.failed_tests}")
        print(f"æˆåŠŸç‡:         {report.success_rate:.1f}%")
        print(f"")
        print(f"å¹³å‡å»¶è¿Ÿ:       {report.avg_latency_ms:.2f} æ¯«ç§’")
        print(f"æœ€å°å»¶è¿Ÿ:       {report.min_latency_ms:.2f} æ¯«ç§’")
        print(f"æœ€å¤§å»¶è¿Ÿ:       {report.max_latency_ms:.2f} æ¯«ç§’")
        print(f"å»¶è¿Ÿæ ‡å‡†å·®:     {report.std_dev_ms:.2f} æ¯«ç§’")
        print(f"")
        print(f"å®é™…é¢‘ç‡:       {report.actual_frequency_hz:.1f} Hz")
        print(f"ç›®æ ‡é¢‘ç‡:       {report.target_frequency_hz:.1f} Hz")
        print(f"é¢‘ç‡è¾¾æˆç‡:     {(report.actual_frequency_hz/report.target_frequency_hz*100):.1f}%")
        print(f"æµ‹è¯•æ—¶é•¿:       {report.total_duration_s:.1f} ç§’")

    def plot_results(self, reports: List[FrequencyTestReport], stability_data: Optional[Dict[str, Any]] = None):
        """ç»˜åˆ¶æµ‹è¯•ç»“æœå›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Revo1 Communication Frequency Test Results', fontsize=16)

        # 1. é¢‘ç‡å¯¹æ¯”å›¾
        ax1 = axes[0, 0]
        function_names = [r.function_name for r in reports]
        actual_frequencies = [r.actual_frequency_hz for r in reports]
        target_frequencies = [r.target_frequency_hz for r in reports]

        x = range(len(function_names))
        width = 0.35

        ax1.bar([i - width/2 for i in x], actual_frequencies, width, label='Actual Frequency', alpha=0.8)
        ax1.bar([i + width/2 for i in x], target_frequencies, width, label='Target Frequency', alpha=0.8)
        ax1.set_xlabel('Test Function')
        ax1.set_ylabel('Frequency (Hz)')
        ax1.set_title('Frequency Comparison')
        ax1.set_xticks(x)
        ax1.set_xticklabels(function_names, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. å»¶è¿Ÿåˆ†å¸ƒå›¾
        ax2 = axes[0, 1]
        avg_latencies = [r.avg_latency_ms for r in reports]
        std_devs = [r.std_dev_ms for r in reports]

        ax2.bar(function_names, avg_latencies, yerr=std_devs, capsize=5, alpha=0.8)
        ax2.set_xlabel('Test Function')
        ax2.set_ylabel('Latency (ms)')
        ax2.set_title('Average Latency with Standard Deviation')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)

        # 3. æˆåŠŸç‡å›¾
        ax3 = axes[1, 0]
        success_rates = [r.success_rate for r in reports]
        colors = ['green' if rate >= 95 else 'orange' if rate >= 90 else 'red' for rate in success_rates]

        bars = ax3.bar(function_names, success_rates, color=colors, alpha=0.8)
        ax3.set_xlabel('Test Function')
        ax3.set_ylabel('Success Rate (%)')
        ax3.set_title('Communication Success Rate')
        ax3.set_ylim(0, 105)
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(True, alpha=0.3)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, success_rates):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{rate:.1f}%', ha='center', va='bottom')

        # 4. ç¨³å®šæ€§æµ‹è¯•ç»“æœ
        ax4 = axes[1, 1]
        if stability_data:
            timestamps = stability_data['timestamps']
            avg_latencies = stability_data['avg_latencies']
            error_rates = stability_data['error_rates']

            # åŒYè½´æ˜¾ç¤ºå»¶è¿Ÿå’Œé”™è¯¯ç‡
            ax4_twin = ax4.twinx()

            line1 = ax4.plot(timestamps, avg_latencies, 'b-', label='Average Latency')
            ax4.set_xlabel('Time (s)')
            ax4.set_ylabel('Latency (ms)', color='b')
            ax4.tick_params(axis='y', labelcolor='b')

            line2 = ax4_twin.plot(timestamps, error_rates, 'r-', label='Error Rate')
            ax4_twin.set_ylabel('Error Rate (%)', color='r')
            ax4_twin.tick_params(axis='y', labelcolor='r')

            # åˆå¹¶å›¾ä¾‹
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax4.legend(lines, labels, loc='upper left')
            ax4.set_title('Long-term Stability Test')
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'No Stability Test Data', ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('Stability Test')

        plt.tight_layout()
        plt.savefig('revo1_comm_frequency_test_results.png', dpi=300, bbox_inches='tight')
        plt.show()

    def save_detailed_results(self, reports: List[FrequencyTestReport], stability_data: Optional[Dict[str, Any]] = None):
        """ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_data = {
            'timestamp': timestamp,
            'test_config': {
                'test_duration': self.config.test_duration,
                'target_frequency': self.config.target_frequency,
                'max_test_count': self.config.max_test_count,
                'min_interval_ms': self.config.min_interval_ms
            },
            'reports': []
        }

        for report in reports:
            report_data['reports'].append({
                'function_name': report.function_name,
                'total_tests': report.total_tests,
                'successful_tests': report.successful_tests,
                'failed_tests': report.failed_tests,
                'success_rate': report.success_rate,
                'avg_latency_ms': report.avg_latency_ms,
                'max_latency_ms': report.max_latency_ms,
                'min_latency_ms': report.min_latency_ms,
                'std_dev_ms': report.std_dev_ms,
                'actual_frequency_hz': report.actual_frequency_hz,
                'target_frequency_hz': report.target_frequency_hz,
                'total_duration_s': report.total_duration_s
            })

        if stability_data:
            report_data['stability_test'] = stability_data

        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(f'revo1_comm_test_report_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: revo1_comm_test_report_{timestamp}.json")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¤– Revo1çµå·§æ‰‹é€šè®¯é¢‘ç‡æµ‹è¯•")
    print("="*60)

    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        test_duration=15.0,          # æ¯ä¸ªæµ‹è¯•15ç§’
        target_frequency=100,        # ç›®æ ‡100Hz
        min_interval_ms=1.0,         # æœ€å°1msé—´éš”
        stability_duration=30.0      # ç¨³å®šæ€§æµ‹è¯•30ç§’
    )

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = Revo1CommTester(config)

    try:
        # åˆå§‹åŒ–è¿æ¥
        if not await tester.initialize():
            return

        # è¿è¡Œå„é¡¹æµ‹è¯•
        reports = []

        # 1. æµ‹è¯• get_motor_status é¢‘ç‡
        report1 = await tester.test_get_motor_status_frequency()
        reports.append(report1)
        tester.print_report(report1)

        # 2. æµ‹è¯• set_finger_positions é¢‘ç‡
        report2 = await tester.test_set_finger_positions_frequency()
        reports.append(report2)
        tester.print_report(report2)

        # 3. æµ‹è¯•æ··åˆåŠŸèƒ½é¢‘ç‡
        report3 = await tester.test_mixed_frequency()
        reports.append(report3)
        tester.print_report(report3)

        # 4. é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•
        print(f"\n{'='*60}")
        print("ğŸ”„ å¼€å§‹é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•...")
        stability_data = await tester.test_stability()

        # æ‰“å°ç¨³å®šæ€§æµ‹è¯•ç»“æœ
        print(f"\n{'='*60}")
        print("ğŸ“Š ç¨³å®šæ€§æµ‹è¯•ç»“æœ")
        print(f"{'='*60}")
        if stability_data['avg_latencies']:
            overall_avg_latency = statistics.mean(stability_data['avg_latencies'])
            overall_error_rate = statistics.mean(stability_data['error_rates'])
            max_error_rate = max(stability_data['error_rates'])
            print(f"å¹³å‡å»¶è¿Ÿ:       {overall_avg_latency:.2f} æ¯«ç§’")
            print(f"å¹³å‡é”™è¯¯ç‡:     {overall_error_rate:.2f}%")
            print(f"æœ€å¤§é”™è¯¯ç‡:     {max_error_rate:.2f}%")
            print(f"é‡‡æ ·ç‚¹æ•°:       {len(stability_data['samples'])}")

        # ç”Ÿæˆå›¾è¡¨å’Œä¿å­˜ç»“æœ
        tester.plot_results(reports, stability_data)
        tester.save_detailed_results(reports, stability_data)

        print(f"\n{'='*60}")
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("ğŸ“Š ç»“æœå›¾è¡¨å·²ä¿å­˜ä¸º: revo1_comm_frequency_test_results.png")
        print(f"{'='*60}")

    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        await tester.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
